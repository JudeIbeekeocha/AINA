{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#A.I.N.A. 1.55"
      ],
      "metadata": {
        "id": "8X_n_1lJeUYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Artificially Intelligent News Anchor (A.I.N.A.)** is an AI that can get current news, summarize them, and tell them to you. The inspiration behind AINA came from my desire to keep up with financial news but the disinterest in browsing different sources. Additionally, when I tried new sources or podcasts that spoke about financial news they did not give me the types of information I desired. So, I decided to create AINA, a way to to quickly consume financial news in one place."
      ],
      "metadata": {
        "id": "1Jc5mFncLFMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Version 1.55 updates:**\n",
        "\n",
        "This version introduces a **new summarization function** with the ability of taking in any article regardless of size/length and summarizing its content. AINA 1.55 also fixes some of the errors/bugs from previous version(s).\n",
        "\n",
        "Fixes errors such as:\n",
        "  * Actual vs. Target\n",
        "  * Summarization function breaking\n",
        "  * Latest funnction `while loop`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zjCMrO18KLjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Constructing A.I.N.A."
      ],
      "metadata": {
        "id": "7JNHa2HzOL9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H38plkpz49yt",
        "outputId": "d9a8ab26-fdbf-4766-8fc4-8a1b9ff27543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.3-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.7.4)\n",
            "Downloading gTTS-2.5.3-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "\n",
        "from copy import copy\n",
        "\n",
        "import requests\n",
        "\n",
        "# Web scraping\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#Transformer\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "# Natural Language ToolKit\n",
        "import nltk\n",
        "\n",
        "# Audio\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio"
      ],
      "metadata": {
        "id": "69K-Vu1sscij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AINA:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------#\n",
        "\n",
        "  def get_story(self, urls):\n",
        "\n",
        "    titles = []\n",
        "    body = []\n",
        "\n",
        "    for index, url in enumerate(urls):\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        # Fetch the article\n",
        "        html_content = response.text\n",
        "\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Extract the title of the article by looking for the h1 tag\n",
        "        title_tag = soup.find('title') #title instead of h1\n",
        "        if title_tag:\n",
        "            titles.append(title_tag.get_text())\n",
        "        else:\n",
        "            titles.append(\"No title found\")\n",
        "\n",
        "        # Extract the main content of the article\n",
        "        article_body = soup.find_all('p')\n",
        "        body_content = [paragraph.get_text() for paragraph in article_body[:-3]]\n",
        "        body.append(' '.join(body_content) if body_content else \"No content found\")\n",
        "\n",
        "    return titles, body\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------#\n",
        "\n",
        "  def latest_news(self, latest=True):\n",
        "    article_location = 0\n",
        "\n",
        "    if not latest:\n",
        "        return [], [], []\n",
        "\n",
        "    # Define headers\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "\n",
        "    # Get the latest news page\n",
        "    response = requests.get('https://finance.yahoo.com/topic/latest-news/', headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        return [], [], []\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract relevant article links\n",
        "    article_links = [a.get('href') for a in soup.find_all('a', href=True) if '/news/' in a.get('href') and '.html' in a.get('href')]\n",
        "    unique_links = []\n",
        "    for link in article_links:\n",
        "      if link not in unique_links:\n",
        "        unique_links.append(link)\n",
        "    for l in unique_links:\n",
        "      if l[:4] != 'http':\n",
        "       unique_links[unique_links.index(l)] = 'https://finance.yahoo.com' + l\n",
        "\n",
        "    # Fetch article details\n",
        "    titles, bodies = [], []\n",
        "    for url in unique_links:\n",
        "        article_response = requests.get(url, headers=headers)\n",
        "        if article_response.status_code != 200:\n",
        "            titles.append(\"No title found\")\n",
        "            bodies.append(\"No content found\")\n",
        "            continue\n",
        "\n",
        "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "        # Extract title\n",
        "        title_tag = article_soup.find('title')\n",
        "        titles.append(title_tag.get_text() if title_tag else \"No title found\")\n",
        "\n",
        "        # Extract body content\n",
        "        article_body = article_soup.find_all('p')\n",
        "        body_content = [paragraph.get_text() for paragraph in article_body[:-3]]\n",
        "        bodies.append(' '.join(body_content) if body_content else \"No content found\")\n",
        "\n",
        "\n",
        "    search = 'No content found'\n",
        "    body = bodies[article_location]\n",
        "\n",
        "    while body == search:\n",
        "      article_location += 1\n",
        "      body = bodies[article_location]\n",
        "      if body != search:\n",
        "        break\n",
        "\n",
        "    return [unique_links[article_location]], [titles[article_location]], [body]\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------#\n",
        "\n",
        "  def general_news(self, article_amount):\n",
        "    links = []\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "\n",
        "    response = requests.get('https://www.yahoo.com/news/', headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        return [], [], []\n",
        "\n",
        "    # Fetch the article\n",
        "    html_content = response.text\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Extract the main content of the article\n",
        "    article_body = soup.find_all('a', href=True)\n",
        "\n",
        "    # Filter out the relevant links\n",
        "    article_links = [link.get('href') for link in article_body if '/news/' in link.get('href')]# this is the source of the problems\n",
        "    specific_links = ['https://www.yahoo.com/news/' + link for link in article_links if '.html' in link]\n",
        "\n",
        "    # Remove duplicates\n",
        "    # links = list(set(specific_links))\n",
        "    links=[]\n",
        "    for link in specific_links:\n",
        "      if link not in links:\n",
        "        links.append(link)\n",
        "    # Limit to the requested number of articles\n",
        "    links = links[:article_amount]\n",
        "\n",
        "    # Fetch the articles and their titles\n",
        "    titles = []\n",
        "    body = []\n",
        "\n",
        "    for url in links:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            titles.append(\"No title found\")\n",
        "            body.append(\"No content found\")\n",
        "            continue\n",
        "\n",
        "        html_content = response.text\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Extract the title of the article\n",
        "        title_tag = soup.find('title')\n",
        "        if title_tag:\n",
        "            titles.append(title_tag.get_text())\n",
        "        else:\n",
        "            titles.append(\"No title found\")\n",
        "\n",
        "        # Extract the main content of the article\n",
        "        article_body = soup.find_all('p')\n",
        "        body_content = [paragraph.get_text() for paragraph in article_body]\n",
        "        body.append(' '.join(body_content) if body_content else \"No content found\")\n",
        "\n",
        "    return links, titles, body\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------#\n",
        "\n",
        "  def related_news(self, tickers_list):\n",
        "\n",
        "    links = []\n",
        "    titles = []\n",
        "    publisher = []\n",
        "\n",
        "    # Collect news articles\n",
        "    for tick in tickers_list:\n",
        "        news = yf.Ticker(tick).news\n",
        "        if news:\n",
        "            links.append(news[0]['link'])\n",
        "            titles.append(news[0]['title'])\n",
        "            publisher.append(news[0]['publisher'])\n",
        "\n",
        "    if len(tickers_list) > 1:\n",
        "      title_list = titles.copy()\n",
        "      link_list = links.copy()\n",
        "      publisher_list = publisher.copy()\n",
        "\n",
        "      actual_title = list(set(titles))  # Get unique titles\n",
        "\n",
        "      search_dup = 'Duplicate'\n",
        "      target = len(tickers_list)\n",
        "      actual = len(actual_title)\n",
        "\n",
        "      article_loc = 1\n",
        "\n",
        "      while actual < target:\n",
        "          title_list_copy = title_list.copy()\n",
        "\n",
        "          for i in range(len(titles)):\n",
        "              if len(title_list_copy) > 0:\n",
        "                  title_list_copy.pop(0)\n",
        "              else:\n",
        "                  break\n",
        "              if titles[i] in title_list_copy:\n",
        "                  title_list[i] = search_dup\n",
        "\n",
        "          find_dup = np.where(np.array(title_list) == search_dup)  # Find duplicates\n",
        "          while search_dup in title_list:\n",
        "              for i in find_dup[0]:\n",
        "                  try:\n",
        "                      news = yf.Ticker(tickers_list[i]).news\n",
        "                      if len(news) > article_loc:\n",
        "                          title_list[i] = news[article_loc]['title']\n",
        "                          link_list[i] = news[article_loc]['link']\n",
        "                          publisher_list[i] = news[article_loc]['publisher']\n",
        "                  except IndexError:\n",
        "                      title_list[i] = 'No more articles'\n",
        "                  except Exception as e:\n",
        "                      print(f\"Error fetching news for {tickers_list[i]}: {e}\")\n",
        "\n",
        "          article_loc += 1\n",
        "          actual = len(set(title_list))  # Recalculate unique titles\n",
        "      links = link_list\n",
        "      titles = title_list\n",
        "      publisher = publisher_list\n",
        "\n",
        "  ##------------------------------------------------------------------------------##\n",
        "\n",
        "      ### This is to get the stories and titles from the links ###\n",
        "      title_story = {}\n",
        "      titles = []\n",
        "      body = []\n",
        "\n",
        "    for index, url in enumerate(links):\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        # Fetch the article\n",
        "        html_content = response.text\n",
        "\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Extract the title of the article by looking for the h1 tag\n",
        "        title_tag = soup.find('title') #title instead of h1\n",
        "        if title_tag:\n",
        "            titles.append(title_tag.get_text())\n",
        "        else:\n",
        "            titles.append(\"No title found\")\n",
        "\n",
        "        # Extract the main content of the article\n",
        "        article_body = soup.find_all('p')\n",
        "        body_content = [paragraph.get_text() for paragraph in article_body[:-3]]\n",
        "        body.append(' '.join(body_content) if body_content else \"No content found\")\n",
        "\n",
        "        title_story[titles[index]] = body[index]\n",
        "\n",
        "  ##------------------------------------------------------------------------------##\n",
        "  # This is to search for 'No content found' and replace it with an actual article\n",
        "\n",
        "    search = 'No content found'\n",
        "    target1 = len(tickers_list)\n",
        "    actual1 = len([i for i in body if i != search])\n",
        "\n",
        "    # Create a list in the event 'NO content found' is in body\n",
        "    if actual1 < target1:\n",
        "      body_copy = body.copy()\n",
        "      title_copy = titles.copy()\n",
        "      link_copy = links.copy()\n",
        "\n",
        "      article_position = article_loc + 1 ## Can start from article_loc from previous if statement\n",
        "\n",
        "      while actual1 < target1:\n",
        "        finder1 = np.where(np.char.startswith(body, search)) # Find where the 'NCF' is/are\n",
        "        for i in finder1[0]:\n",
        "          link = [yf.Ticker(tickers_list[i]).news[article_position]['link']]\n",
        "          title_story, body_story = self.get_story(link)\n",
        "          body_copy[i] = body_story[0]\n",
        "          title_copy[i] = title_story[0]\n",
        "          link_copy[i] = link[0]\n",
        "\n",
        "        article_position += 1\n",
        "        actual1 = len([i for i in body_copy if i != search])\n",
        "        print(f'Target: {target1} | Actual: {actual1}')\n",
        "\n",
        "      body = body_copy\n",
        "      titles = title_copy\n",
        "      links = link_copy\n",
        "    else:\n",
        "      pass\n",
        "    return links, titles, body\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------#\n",
        "\n",
        "  def call_news_functions(self, general_amt, tickers_list, latest = True):\n",
        "    url_specific, title_specific, body_specific = self.latest_news(latest)\n",
        "    url_news, title_news, body_news = self.general_news(general_amt)\n",
        "    link_related, title_related, body_related = self.related_news(tickers_list)\n",
        "\n",
        "    title_story = {}\n",
        "\n",
        "    link = []\n",
        "    [link.extend(i) for i in [url_specific, url_news, link_related]]\n",
        "\n",
        "    body = []\n",
        "    [body.extend(i) for i in [body_specific, body_news, body_related]]\n",
        "\n",
        "    title = []\n",
        "    [title.extend(i) for i in [title_specific, title_news, title_related]]\n",
        "\n",
        "    for i in range(len(link)):\n",
        "      title_story[title[i]] = body[i]\n",
        "\n",
        "    return link, title_story\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------#\n",
        "\n",
        "  def summarize(self, story_dict):\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    checkpoint = 'facebook/bart-large-cnn'\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "    summary = {}\n",
        "\n",
        "    try:\n",
        "      for title, story in story_dict.items():\n",
        "        sentences = nltk.tokenize.sent_tokenize(story)\n",
        "\n",
        "\n",
        "        length = 0\n",
        "        chunk = \"\"\n",
        "        chunks = []\n",
        "        count = -1\n",
        "        context = 1024\n",
        "        for sentence in sentences:\n",
        "          count += 1\n",
        "          combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
        "\n",
        "          if combined_length  <= context: # if it doesn't exceed\n",
        "            chunk += sentence + \" \" # add the sentence to the chunk\n",
        "            length = combined_length # update the length counter\n",
        "\n",
        "            # if it is the last sentence\n",
        "            if count == len(sentences) - 1:\n",
        "              chunks.append(chunk.strip()) # save the chunk\n",
        "\n",
        "          else:\n",
        "            chunks.append(chunk.strip()) # save the chunk\n",
        "\n",
        "            # reset\n",
        "            length = 0\n",
        "            chunk = \"\"\n",
        "\n",
        "            # take care of the overflow sentence\n",
        "            chunk += sentence + \" \"\n",
        "            length = len(tokenizer.tokenize(sentence))\n",
        "        sum = ''\n",
        "        for chunk in chunks:\n",
        "          summarizer = pipeline(\"summarization\", model = checkpoint)\n",
        "          sum += summarizer(chunk, min_length=100, max_length=125)[0]['summary_text']\n",
        "        summary[title] = sum\n",
        "    except Exception as e:\n",
        "      print(f'Could not produce summary: {e}')\n",
        "    return summary\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------#\n",
        "\n",
        "  def sanitize_filename(self, title): # This get's rid of any special charactes in the title allowing the file to be saved.\n",
        "    # Replace invalid characters with underscores\n",
        "    return ''.join(c if c.isalnum() or c in (' ', '_') else '_' for c in title)\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------#\n",
        "\n",
        "  def voice(self, summarized):\n",
        "    # Iterate over the dictionary and convert each story to speech\n",
        "    for title, story in summarized.items():\n",
        "        print(f\"Title: '{title}'\")\n",
        "        tts = gTTS(text=story, lang='en')\n",
        "        # Replace invalid characters in the title for file naming\n",
        "\n",
        "        title = self.sanitize_filename(title)\n",
        "        # Save the audio file\n",
        "        audio_file = f\"{title}.mp3\"\n",
        "        tts.save(audio_file)\n",
        "\n",
        "        # Play the audio file\n",
        "        display(Audio(audio_file))\n"
      ],
      "metadata": {
        "id": "6YlNXf8NUGyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calling A.I.N.A."
      ],
      "metadata": {
        "id": "-tM_Ht7TOEw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  tickers = [ '^GSPC', 'MSFT', 'AAPL', 'GOOG']\n",
        "  aina = AINA()\n",
        "  link, story = aina.call_news_functions(2, tickers, True)\n",
        "  summary = aina.summarize(story)\n",
        "  aina.voice(summary)"
      ],
      "metadata": {
        "id": "8ra_WStbsZVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ua9-TAdis2UN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}